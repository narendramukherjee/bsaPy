\documentclass[11pt]{article}

\usepackage[margin=1.0in]{geometry}
\usepackage{url, enumitem}
\usepackage{amsfonts, amsmath, amsthm, amssymb}
\usepackage{listings}
\usepackage{hyperref}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\theoremstyle{plain}
\usepackage[textsize=tiny]{todonotes}

% Some useful macros.
\newcommand{\given}{\,|\,}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}{\text{var}}
\newcommand{\cov}{\text{cov}}
\newcommand{\p}{\partial}
\newcommand{\mba}{\mathbf{a}}
\newcommand{\mbb}{\mathbf{b}}
\newcommand{\mbx}{\mathbf{x}}
\newcommand{\mcX}{\mathcal{X}}
\newcommand{\mcY}{\mathcal{Y}}
\newcommand{\boldw}{\mathbf{w}}
\newcommand{\mbxt}{\tilde{\mathbf{x}}}
\newcommand{\Sigmat}{\tilde{\Sigma}}
\newcommand{\mbz}{\mathbf{z}}
\newcommand{\mbw}{\mathbf{w}}
\newcommand{\mcN}{\mathcal{N}}
\newcommand{\mcP}{\mathcal{P}}
\newcommand{\eps}{\epsilon}
\newcommand{\trans}{\intercal}
\newcommand{\Ut}{\tilde{U}}
\newcommand{\Beta}{\text{Beta}}
\newcommand{\Bernoulli}{\text{Bernoulli}}
\newcommand{\Elbo}{\text{ELBO}}
\newcommand{\KL}{\text{KL}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\angstrom}{\textup{\AA}}
\renewcommand{\v}[1]{\mathbf{#1}}


\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta
    urlcolor=cyan,
}



\begin{document}
\begin{center}
Mark Goldstein and Narendra Mukherjee: 6.882 project model 
\end{center}
\section{Introduction}

\noindent Let $\mathbf{X}$ be signal data segmented into $N$ time windows of $\ell$ samples each.
We are interested in inferring the frequency components present in each time window. 
To do this, we will use a non-parametric latent feature model with a Beta Process prior. In the generative story,
each time window of the signal exhibits a blend of several latent features. Each of these latent features is defined by
a $\sin$ at a particular frequency. There are an infinite amount of such $\sin$s. Each time window exhibits a finite amount
of these $\sin$s.\\

\noindent Let $\mathbf{X} \in \mathbb{R}^{D=1 \times N}$ be a matrix of the time windows. 
We set the dimension $D$ of each time window to $D=1$ 
and consider each time window as an indivisible scalar-like entity for the purposes
of the following setup, though each window is actually an array of $\ell$ samples. 
The only place where we will need to consider the underlying samples is in the likelihood term.
Otherwise, we reason only at the window level by assigning blends of frequencies to windows, where
each frequency is also $\ell$ samples.\\

\noindent Let $\mathbf{\Phi} \in \mathbb{R}^{D=1 \times K}$ be a matrix of parameters for a set of $K$ 
signal basis elements to be allocated as latent features to the time windows.
Each $\mathbf{\phi}_k$ is the log of the frequency parameter for the $k^{th}$ basis element, 
$\sin(\exp[\phi_k])$. We misuse notation by considering a matrix product with 
$\boldsymbol{\Phi}$ to be a combination of $\sin(\exp[\phi_k])$'s rather than a combination 
of $\phi_k$'s. So if $\mathbf{a} \in \mathbb{R}^K$, then we let $\boldsymbol{\Phi}\mathbf{a} =
\sum_{k=1}^K a_k \sin(\exp[\phi_k])$. Let $\mathbf{Z} \in \mathbb{R}^{K \times N}$ be a binary matrix 
indicating the presence of the $k^{th}$ latent feature in the $i^{th}$ time window of the signal 
with $k \in \{1 \ldots K\}$ and $i \in \{1 \ldots N\}$. Let $\mathbf{W} \in \mathbb{R}^{K \times N}$ 
be the weights of the $k^{th}$ latent feature in the $i^{th}$ time window. Let $\mathbf{E} 
\in \mathbb{R}^{D \times N}$ be a noise matrix, such that $X = \mathbf{\Phi}(\mathbf{Z} \circ 
\mathbf{W}) + \mathbf{E}$ with $x_i = \mathbf{\Phi}(\mathbf{z}_i \circ \mathbf{w}_i) + \epsilon_i$. 
The generative model is:

\begin{align*}
    \epsilon_i &\sim \mathcal{N}(0, \sigma^2_i)\\
    w_{ik} &\sim \mathcal{N}(0, \sigma^2_w)\\
    \phi_k &\sim \mathcal{N}(\mu_{\phi}, \sigma^2_{\phi})\\
    z_{ik} &\sim \Bernoulli(\pi_k)\\
    \pi_k &\sim \Beta(a/K, b(K-1)/K)
\end{align*}

\noindent This corresponds to a Beta Process prior on $Z$. The logs of the frequency parameters 
$\phi_k$ are drawn $i.i.d$ from base distribution $H_0$, in this case the Normal.
The $\pi_k$'s are drawn from $\Beta(a/K,b(K=1)/K)$. This gives us $H = \sum_{k=1}^K \pi_k \delta_{\phi_k} \sim BP(a,b,H_0)$ as $K \rightarrow \infty$. (Paisley 2009).\\

\noindent Our likelihood model is that each window $i$ of the true signal, now to be thought of as $\ell$-samples-dimensional, is normally distributed around the $i^{th}$
time window of the estimated signal:

\begin{align*}
    \hat{\mathbf{x}}_i &= \sum_{k=1}^K z_{ik}w_{ik}\big[\sin\big(\exp[\phi_k]\big)\big]\\
    p(\mathbf{X} |\mathbf{Z}, \mathbf{W}, \boldsymbol{\Phi},\{\sigma^2_i\}_{i=1}^N) &= \sum_{i=1}^N \log \mathcal{N}(\mathbf{x}_i | \hat{\mathbf{x}}_i, \sigma^2_i\mathbf{I})
\end{align*}


\section{Variational Inference Overview}

\noindent Let $\mathbf{B} = \{\boldsymbol{\pi}, \mathbf{Z}, \mathbf{\Phi}, \mathbf{W}\}$ and 
$\boldsymbol{\theta} = \{\sigma^2_i, \sigma^2_w, \mu_{\phi}, \sigma^2_\phi, a, b \}$. 
We will approximate the true posterior $p(\mathbf{B}|\mathbf{X},\boldsymbol{\theta})$, intractable because of the \textit{evidence} $p(X)$,
with a variational distribution $q(\mathbf{B})$ that minimizes the divergence $\KL(q(\mathbf{B}) || p(\mathbf{B}|\mathbf{X},\boldsymbol{\theta})$. 
Let our variational distribution take the form $q_{\boldsymbol{\eta}}(\mathbf{B}) = q_{\boldsymbol{\tau}}(\boldsymbol{\pi}) q_{\boldsymbol{\gamma}}(\boldsymbol{\Phi}) 
q_{\boldsymbol{\lambda}}(\mathbf{W}) q_{\boldsymbol{\nu}}(\mathbf{Z})$. We optimize $q$'s parameters $\boldsymbol{\eta} = \{\boldsymbol{\tau}, \boldsymbol{\gamma}, 
\boldsymbol{\lambda}, \boldsymbol{\nu}\}$ to most closely match the true posterior. Omit conditioning on the hyperparameters $\boldsymbol{\theta}$ for brevity. Minimizing 


\begin{align*}
    \KL(q(\mathbf{B})||p(\mathbf{B}|\mathbf{X})) &= \E_q[\log q(\mathbf{B})] - \E_q[\log p(\mathbf{B,X})] + \log p(\mathbf{X})
\end{align*}

\noindent still depends on the intractable term $p(\mathbf{X})$. We drop $p(\mathbf{X})$ because
it does not depend on $q$. We maximize the negative of the leftover terms

\begin{align*}
    \Elbo[q] = \E_q[\log p(\mathbf{B,X})] - \E_q[\log q(\mathbf{B})] 
\end{align*}

\noindent The first term is the joint of the data and the model and
the second term is the entropy $H[q]$ of the variational distribution. 
We expand the first term and get 

\begin{align*}
    \Elbo[q] = \E_q[\log p(\mathbf{B})] + \E_q[\log p(\mathbf{X}|\mathbf{B})] - H[q]
\end{align*}

\noindent The first term considers the priors, the middle term the likelihood, and the last term
the entropy of the variational distribution. This also equals 

\begin{align*}
    \Elbo[q] = \E_q[\log p(\mathbf{X}|\mathbf{B})] - \KL(q(\mathbf{B}) || p(\mathbf{B}))
\end{align*}

\noindent which makes explicit that we pick a $q$ that gives us high likelihood but that stays close to the priors. 
This optimization is non-convex and we are only guaranteed to find local optima. We cycle through each variational parameter, and perform coordinate ascent 
to optimize the ELBO. The ELBO has its name because this is equivalent to maximizing a lower bound on the evidence (also called marginal likelihood)\\

\begin{align*}
    q_{\tau_k}(\pi_k) &= \Beta(\pi_k|\tau_k^a, \tau_k^b)\\
    q_{\gamma_k}(\phi_k) &= \mathcal{N}(\gamma_{k}^{\mu}, \gamma_{k}^{\sigma^2})\\
    q_{\lambda_{ik}}(w_{ik}) &= \mathcal{N}(\lambda_{ik}^{\mu}, \lambda_{ik}^{\sigma^2})\\
    q_{\nu_{ik}}(z_{ik}) &= \Bernoulli(z_{ik}|\nu_{ik})
\end{align*}

\section{Expanding the ELBO:}

\begin{align*}
    \log p (\mathbf{X}|\boldsymbol{\theta}) &\geq \E_{q(\mathbf{B})}[\log p(\mathbf{X,B} | \boldsymbol{\theta})] + H[q]\\
                                       &= \sum_{k=1}^K \E_{q(\pi_k)}[\log p(\pi_k|a,b)] 
    + \sum_{i=1}^N \sum_{k=1}^K \E_{q(\pi_k),q(z_{ik})}[\log p(z_{ik}|\pi_k)]\\
    &\quad + \sum_{k=1}^K \E_{q(\phi_k)}[\log p(\phi_k|\mu_{\phi},\sigma^2_\phi)] + \sum_{k=1}^K \E_{q(w_{ik})}[ \log p(w_{ik}|\sigma^2_w)]\\
    &\quad + \sum_{i=1}^N \E_{q(z_i),q(\phi),q(w_i)}[\log p(x_i | \mathbf{z_i}, \boldsymbol{\phi}, \mathbf{w}_i, \sigma^2_i)] + H[q]
\end{align*}

\noindent Next, we must substitute in the actual distributions for each of these terms.
We now simplify each term separately before presenting the whole $ELBO$ in its expanded form.

\subsection{$\E_{q(\pi_k)}[\log p(\pi_k|a,b)]$}

\noindent The Beta PDF is $\Beta(x|a,b) = \frac{x^{a-1}(1-x)^{b-1}}{B(a,b)}$ 
where $B(a,b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$. The Gamma function $\Gamma()$
has the property that $\Gamma(x+1) =  (x)\Gamma(x)$. Let $\psi()$ be the digamma function.
$\psi(x) = \frac{d}{dx} \log \Gamma(x) = \frac{\Gamma^\prime(x)}{\Gamma(x)}$. 


%Doshi-Velez 2009 shows the following 
%relevant derivation, for $p(\pi) = \Beta(a/K,1)$ and $q(\pi) = \Beta(\pi_k|\tau_k^a,\tau_k^b)$:
%
%\begin{align*}
%    \E_{q(\pi)}[\log p (\pi_k|a)] &= \E_{q(\pi)}\big[ \log \big(\frac{a}{K} \pi_k^{a/k-1} \big) \big]\\
%&= \log \frac{a}{K} + \big(\frac{a}{K} - 1 \big) \E_{q(\pi)} [\log(\pi_k)]\\
%&= \log \frac{a}{K} + \big(\frac{a}{K} - 1 \big) (\psi(\tau_{k}^a) - \psi(\tau_{k}^a + \tau_{k}^b))
%\end{align*}

%In our case, the Beta does not have a simple $1$ for the second parameter. 

\noindent Our distributions are:
\begin{align*}
    p(\pi_k) &= \Beta(\alpha, \beta)\\
    q_{\tau_k}(\pi_k) &= \Beta(\pi_k|\tau_k^a, \tau_k^b)
\end{align*}

\noindent where $\alpha = \frac{a}{K}$ and $\beta= b(K-1)/K$, with $a,b \in \boldsymbol{\theta}$
from the original model specification.

\begin{align*}
    \E_{q(\pi_k)}[\log p(\pi_k|\alpha,\beta)] &= 
        \E_{q(\pi_k)}[\log \big( \frac{\pi_k^{\alpha-1}(1-\pi_k)^{\beta-1}}
        {\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}}\big) ]\\   
    &= \E_{q(\pi_k)}[\log \big(\pi_k^{\alpha-1}(1-\pi_k)^{\beta-1} 
        \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \big)]\\
    &= \E_{q(\pi_k)}[ (\alpha-1)\log(\pi_k) + (\beta-1)\log(1 - \pi_k)
        + \log \Gamma(\alpha+\beta) - \log \Gamma(\alpha) - \log \Gamma(\beta)]\\
    &= (\alpha-1)\E_{q(\pi_k)}[\log(\pi_k)] + (\beta-1)\E_{q(\pi)}[\log(1 - \pi)]
        + \log \Gamma(\alpha+\beta) - \log \Gamma(\alpha) - \log \Gamma(\beta)\\
    &= (\alpha-1)(\psi(\tau_{k}^a) - \psi(\tau_{k}^a + \tau_{k}^b))
      + (\beta-1)(\psi(\tau_{k}^b) - \psi(\tau_{k}^a + \tau_{k}^b))\\
      &\quad + \log \Gamma(\alpha+\beta) - \log \Gamma(\alpha) - \log \Gamma(\beta)\\
\end{align*}

\noindent The above uses the property that if $q(\pi_k) = \Beta(\pi_k|\tau_k^a,\tau_k^b)$
then $\E_{q(\pi_k)}[\log \pi_k] = \psi(\tau_k^a) - \psi(\tau_k^a + \tau_k^b)$
and $\E_{q_(\pi_k)}[\log (1 - \pi_k)] = \psi(\tau_k^b) - \psi(\tau_k^a + \tau_l^b)$.
This comes from the property that $\pi_k \sim \Beta(\tau_k^a,\tau_k^b)$ means
that $1 - \pi_k$ is distributed $\Beta(\tau_k^b,\tau_k^a)$. The constants that
do not depend on $q$ will come out in the derivations of the coordinate ascent updates.

\subsection{$\E_{q(\pi_k),q(z_{ik})}[\log p(z_{ik}|\pi_k)]$}

\noindent Recall that $q(z) = \Bernoulli(z_ik|\nu_{ik})$ and $q(\pi_k) = \Beta(\pi_k|\tau_k^a,\tau_k^b)$

\begin{align*}
    \E_{q(\pi_k),q(Z)}[\log p (z_{nk}|\pi_k)] 
        &= \E_{q(\pi_k),q(Z)}\big[\log \big(\pi_k^{z_{ik}}(1 - \pi_k)^{1 - z_{ik}}\big)\big]\\
        &= \E_{q(\pi_k),q(Z)}\big[ z_{ik} \log \pi_k + (1 - z_{ik})\log(1 - \pi_k) big]\\
        &= \E_{q(Z)}[z_{ik}] \E_{q(\pi_k)}[\log \pi_k] +  
            (1 - \E_{q(Z)}[z_{ik}]) \E_{q(\pi_k)}[\log(1 - \pi_k)]\\
        &= \nu_{ik} \psi(\tau_k^a) + (1 - \nu_{ik})\psi(\tau_k^b) - \psi(\tau_k^a + \tau_k^b)
\end{align*}






\subsection{$\E_{q(\phi_k)}[\log p(\phi_k|\mu_{\phi},\sigma^2_\phi)]$}


\noindent Recall that $p(\phi_k) = \mathcal{N}(\mu_{\phi},\sigma^2_{\phi})$
and $q_{\gamma_k}(\phi_k) = 
\mathcal{N}(\gamma_{k}^{\mu}, \gamma_{k}^{\sigma^2})$\\

\begin{align*}
    \E_{q(\phi_k)}[\log p(\phi_k|\mu_{\phi},\sigma^2_\phi)] &=
        \E_{q(\phi_k)}
            \Big[%expectation
            \log \Big(%log
                \frac{1}{\sqrt{2 \pi \sigma^2_\phi}} \exp 
                \big[-\frac{(\phi_k - \mu_{\phi})}{2 \sigma^2_\phi}\big]
            \Big)%log
            \Big]%expectation
            \\
       &= -\frac{1}{2} \log \Big[2 \pi \sigma^2_{\phi} \Big]
          - \frac{\gamma_{k}^{\sigma^2} + \big(\gamma_k^{\mu} - 
          \mu_{\phi} \big)^2}{2 \sigma^2_{\phi}}
\end{align*}


\subsection{$\E_{q(w_{ik})}[ \log p(w_{ik}|\sigma^2_w)]$}



\noindent Recall that $p(w_{ik}) = \mathcal{N}(0, \sigma^2_w)$
and $q_{\lambda_{ik}}(w_{ik}) = 
\mathcal{N}(\lambda_{ik}^{\mu}, \lambda_{ik}^{\sigma^2})$. Using the result just
above, we get






\begin{align*}
    \E_{q(w_{ik})}[ \log p(w_{ik}|\sigma^2_w)] &=
    - \frac{1}{2} \log \Big[2 \pi \sigma^2_{w} \Big]
              - \frac{\lambda_{ik}^{\sigma^2} 
              + \big(\lambda_{ik}^{\mu} - 0 \big)^2}{2 \sigma^2_{w}}
\end{align*}


\subsection{$\E_{q(z_i),q(\phi),q(w_i)}[\log p(x_i | \mathbf{z_i}, \boldsymbol{\phi}, \mathbf{w}_i, \sigma^2_i)]$ (likelihood term)}

...

\subsection{The expanded ELBO with our concrete choices of $p$'s and $q$'s}

...

\section{Coordinate Ascent Updates}

...


\begin{center} \textbf{References} \end{center}
    \begin{itemize}
        \item Bretthorst. Bayesian Spectrum Analysis and Parameter Estimation. 1988.
        \item Doshi-Velez et al. Variational Inference for the IBP. May (not April) 2009.
        \item Liang and Hoffman. Beta Process Non-negative Matrix Factorization with Stochastic Structured Mean-Field Variational Inference. 2014.
        \item Paisley and Carin. Nonparametric Factor Analysis with Beta Process Priors. 2009.
        \item Paisley, Carin, and Blei. Variational Inference for Stick-Breaking Beta Process Priors. 2011.
        \item Paisley, Blei, and Jordan. Stick-Breaking Beta Processes and the Poisson Process. 2012.
        \item Turner and Sahani. Time-frequency analysis as probabilistic inference. 2014. 
    \end{itemize}
\end{document}
